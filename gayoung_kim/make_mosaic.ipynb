{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### based on https://github.com/jason9075/opencv-mosaic-data-aug by jason9075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "dataset = 'medical'\n",
    "\n",
    "TOTAL = 150\n",
    "OUTPUT_SIZE = (4096, 4096)  # Height, Width\n",
    "SCALE_RANGE = (0.3, 0.7)\n",
    "FILTER_TINY = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = os.path.join('/opt/ml/input/data', dataset, 'img/train')\n",
    "\n",
    "LABEL_DIR = os.path.join('/opt/ml/input/data/', dataset, 'ufo', 'train.json')\n",
    "\n",
    "os.makedirs(os.path.join('/opt/ml/input/data/', 'mosaic', 'img/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join('/opt/ml/input/data/', 'mosaic', 'ufo'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABEL_DIR, 'r', encoding='utf-8') as f:\n",
    "    ufo = json.load(f)['images']\n",
    "ufo_image_names = [x for x in ufo.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img = np.zeros([OUTPUT_SIZE[0], OUTPUT_SIZE[1], 3], dtype=np.uint8)\n",
    "\n",
    "license_tag = dict(usability=True, public=True,\n",
    "                   commercial=True, type='CC-BY-SA',\n",
    "                   holder=None)\n",
    "orientation = 'Horizontal'\n",
    "\n",
    "\n",
    "new_anno = dict()\n",
    "\n",
    "for i in range(TOTAL):\n",
    "    \n",
    "    # scale_x, scale_y는 SCALE_RANGE 사이의 숫자\n",
    "    scale_x = SCALE_RANGE[0] + random.random() * (SCALE_RANGE[1] - SCALE_RANGE[0])\n",
    "    scale_y = SCALE_RANGE[0] + random.random() * (SCALE_RANGE[1] - SCALE_RANGE[0])\n",
    "    divid_point_x = int(scale_x * OUTPUT_SIZE[1])\n",
    "    divid_point_y = int(scale_y * OUTPUT_SIZE[0])\n",
    "\n",
    "    temp_anno = dict(img_h=OUTPUT_SIZE[0], img_w=OUTPUT_SIZE[1],\n",
    "                     tags=None, license_tag=license_tag)\n",
    "    temp_words = dict()\n",
    "    counter = 0\n",
    "    \n",
    "    for j in range(4):\n",
    "        \n",
    "        index = random.randrange(len(ufo))\n",
    "        key = ufo_image_names[index]\n",
    "        value = ufo[key]\n",
    "        img = cv2.imread(os.path.join(IMG_DIR, key))\n",
    "        \n",
    "        img_height, img_width, _ = img.shape\n",
    "        \n",
    "        # top-left\n",
    "        if j == 0:\n",
    "            img = cv2.resize(img, (divid_point_x, divid_point_y))\n",
    "            output_img[:divid_point_y, :divid_point_x, :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "                \n",
    "                a0 = float(int(a0*divid_point_x / img_width))\n",
    "                a1 = float(int(a1*divid_point_y / img_height))\n",
    "                \n",
    "                b0 = float(int(b0*divid_point_x / img_width))\n",
    "                b1 = float(int(b1*divid_point_y / img_height))\n",
    "                \n",
    "                c0 = float(int(c0*divid_point_x / img_width))\n",
    "                c1 = float(int(c1*divid_point_y / img_height))\n",
    "                \n",
    "                d0 = float(int(d0*divid_point_x / img_width))\n",
    "                d1 = float(int(d1*divid_point_y / img_height))\n",
    "\n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        tags=word['tags'],\n",
    "                                        confidence=word['confidence'])\n",
    "                counter += 1\n",
    "\n",
    "        # top-right\n",
    "        elif j == 1:\n",
    "            img = cv2.resize(img, (OUTPUT_SIZE[1] - divid_point_x, divid_point_y))\n",
    "            output_img[:divid_point_y, divid_point_x:OUTPUT_SIZE[1], :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "                \n",
    "                a0 = float(int(divid_point_x + (a0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                a1 = float(int(a1*divid_point_y / img_height))\n",
    "                \n",
    "                b0 = float(int(divid_point_x + (b0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                b1 = float(int(b1*divid_point_y / img_height))\n",
    "                \n",
    "                c0 = float(int(divid_point_x + (c0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                c1 = float(int(c1*divid_point_y / img_height))\n",
    "                \n",
    "                d0 = float(int(divid_point_x + (d0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                d1 = float(int(d1*divid_point_y / img_height))\n",
    "                \n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        tags=word['tags'],\n",
    "                                        confidence=word['confidence'])\n",
    "                counter += 1\n",
    "\n",
    "        # bottom-left\n",
    "        elif j == 2:\n",
    "            img = cv2.resize(img, (divid_point_x, OUTPUT_SIZE[0] - divid_point_y))\n",
    "            output_img[divid_point_y:OUTPUT_SIZE[0], :divid_point_x, :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "    \n",
    "                a0 = float(int(a0*divid_point_x / img_width))\n",
    "                a1 = float(int(divid_point_y + (a1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                b0 = float(int(b0*divid_point_x / img_width))\n",
    "                b1 = float(int(divid_point_y + (b1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                c0 = float(int(c0*divid_point_x / img_width))\n",
    "                c1 = float(int(divid_point_y + (c1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                d0 = float(int(d0*divid_point_x / img_width))\n",
    "                d1 = float(int(divid_point_y + (d1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "\n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        tags=word['tags'],\n",
    "                                        confidence=word['confidence'])\n",
    "                counter += 1\n",
    "\n",
    "        # bottom-right\n",
    "        else:\n",
    "            img = cv2.resize(img, (OUTPUT_SIZE[1] - divid_point_x, OUTPUT_SIZE[0] - divid_point_y))\n",
    "            output_img[divid_point_y:OUTPUT_SIZE[0], divid_point_x:OUTPUT_SIZE[1], :] = img\n",
    "\n",
    "            for word in value['words'].items():\n",
    "                word = word[1]\n",
    "                a0, a1 = word['points'][0][0], word['points'][0][1]\n",
    "                b0, b1 = word['points'][1][0], word['points'][1][1]\n",
    "                c0, c1 = word['points'][2][0], word['points'][2][1]\n",
    "                d0, d1 = word['points'][3][0], word['points'][3][1]\n",
    "                \n",
    "                a0 = float(int(divid_point_x + (a0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                a1 = float(int(divid_point_y + (a1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                b0 = float(int(divid_point_x + (b0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                b1 = float(int(divid_point_y + (b1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                c0 = float(int(divid_point_x + (c0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                c1 = float(int(divid_point_y + (c1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "                \n",
    "                d0 = float(int(divid_point_x + (d0*(OUTPUT_SIZE[1]-divid_point_x) / img_width)))\n",
    "                d1 = float(int(divid_point_y + (d1*(OUTPUT_SIZE[0]-divid_point_y) / img_height)))\n",
    "            \n",
    "                if (c0-a0)*(c1-a1) < FILTER_TINY:\n",
    "                    continue\n",
    "                temp_words[counter] = dict(points=[[a0,a1],[b0,b1],[c0,c1],[d0,d1]],\n",
    "                                        transcription=word['transcription'],\n",
    "                                        language=word['language'],\n",
    "                                        illegibility=word['illegibility'],\n",
    "                                        orientation=word['orientation'],\n",
    "                                        tags=word['tags'],\n",
    "                                        confidence=word['confidence'])\n",
    "                counter += 1\n",
    "    \n",
    "    # save image\n",
    "    image_name = f'mosaic_{i}.jpg'\n",
    "    cv2.imwrite(os.path.join('/opt/ml/input/data/', 'mosaic', 'img/train', image_name), output_img)\n",
    "     \n",
    "    temp_anno['words'] = temp_words\n",
    "    new_anno[image_name] = temp_anno\n",
    "\n",
    "# create ufo train.json\n",
    "anno = dict(images=new_anno)\n",
    "\n",
    "with open(os.path.join('/opt/ml/input/data/', 'mosaic', 'ufo', 'train.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(anno, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
